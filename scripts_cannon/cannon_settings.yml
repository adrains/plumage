# This file contains the common set of parameters used to train and plot 
# diagnostic plots for a Cannon model.
#
# This is part of a series of Cannon scripts. The main sequence is:
# 1) prepare_stannon_training_sample.py    --> label preparation
# 2) train_stannon.py                      --> training and cross validation
# 3) make_stannon_diagnostics.py           --> diagnostic plots + result tables
# 4) run_stannon.py                        --> running on science spectra
#
# Where the values in this file currently apply to items 2) and 3).
#
# YAML specifications: https://yaml.org/spec/1.2.2/

#------------------------------------------------------------------------------
# Model Settings
#------------------------------------------------------------------------------
# Suppress ouput from Stan during training. It is recommended to always have
# this on for the label_uncertainties model since it takes much longer to
# converge than the less complex basic model.
suppress_stan_output: true

# Whether to initialise theta and s2 vectors for a label uncertainty model
# using the vectors from a trained basic model. The idea is that, even though
# these will ultimately be different, it's a better initial guess than just 
# starting with an array of zeroes. Initial testing bears this out, as the log
# probability starting points from a naive initial guess are radically
# different from those informed by a basic model (log prob -500,000 vs +260,000
# for a test case 842 px model)
init_with_basic_model: false

# The maximum amount of iterations Stan will run while fitting the model. The
# label_uncertainty model requires a higher number of iterations to converge,
# though preliminary testing indicates the model is ~mostly converge after the
# first ~10% of iterations and the remaining 90% are "fine tuning"--useful to
# know for testing purposes to save time. Note that the basic model is trained
# pixel-by-pixel with each pixel taking approximately a few hundred iterations
# to converge, so max_iter isn't really relevant here.
max_iter: 500000

# By default Stan only logs a fitting update once every max_iter/10 iterations.
# For large max_iter values, this might not be frequent enough--especially when
# testing--so this can be updated here. 
refresh_rate_frac: 10000

# Whether to run leave-one-out cross validation on Cannon model. If yes, the
# cross validation is done using the same suppress_stan_output, 
# init_with_basic_model, and max_iter settings as the original training.
do_cross_validation: True

# Whether to do sigma clipping using trained Cannon model. If True, an initial
# Cannon model is trained and its model spectra are used to sigma clip bad 
# pixels to not be considered for the subsequently trained and adopted model.
# Note that this can potentially cause unexpected results if the initial
# Cannon model is poorly trained, so best to leave as False when testing.
do_iterative_bad_px_masking: True
flux_sigma_to_clip: 6

# Normalisation - using using a Gaussian smoothed version of the spectrum, or 
# a much simpler polynomial fit. Only wavelengths > than wl_min_normalisation
# will be considered during either approach to avoid low-SNR blue pixels for
# the coolest stars. TODO: save these parameters in the Cannon model.
do_gaussian_spectra_normalisation: True
wl_min_normalisation: 4000
wl_broadening: 50
poly_order: 4

# Minimum and maximum wavelengths for Cannon model
wl_min_model: 4000
wl_max_model: 7000

# The Cannon model to use - either the 'basic' traditional Cannon model, or a
# model with label uncertainties. If modelling abundances, the version with
# label uncertainties should be used. Either 'basic' or 'label_uncertainties'.
# For a 3-term model, the basic model (on motley) takes of order ~1 min to
# train, and the label uncertainties model takes of order ~20 min. The latter 
# increases to ~33 min when training a 4 term model with [Ti/H]. Note that
# these numbers are for a *single* model, and that cross validation increases
# the runtime by a factor of N_stars. TODO: update these numbers.
model_type: "basic"

# For testing purposes, we can run with uniform label variances instead of the
# literature uncertainties. To do this, we assign every label the same
# percentage uncertainty. For instance, uniform_var_frac_error = 0.01 means
# that all labels have a 1% uncertainty. If this is set to False, we instead
# run with the observed label uncertainties.
use_label_uniform_variances: False
uniform_var_frac_error: 0.01

# To constrain the parameter space (or just for testing) when using the label
# uncertainties model, we can rescale the literature uncertainties by a
# constant amount on a per-label basis. For instance, setting 
# lit_std_scale_fac = [0.1, 0.1, 0.1, 0.1] means that we adopt
# uncertainties 10x smaller than observed for each label of a four label model.
# This only takes effect if use_label_uniform_variances is set to False. Set to 
# an array of ones if we don't want to scale the uncertainties.
lit_std_scale_fac: [1.0, 1.0, 1.0, 1.0]

# Also for testing and as a complement to the previous setting we can scale the
# uncertainties on the spectra itself. spectra_std_scale_fac < 1.0 mean that
# we're artifically increasing the SNR of the spectrum.
spectra_std_scale_fac: 1.0

model_save_path: "spectra"
std_label: "cannon"

# Whether to fit for abundances. At the moment our abundance heirarchy is
# Montes+18 > Valenti+Fischer05 > Adibekyan+12. Not recommended to fit > 1-2.
# Available options (for Montes+18, which is the most complete): 
# Na, Mg, Al, Si, Ca, Sc, Ti, V, Cr, Mn, Co, Ni
# Select as e.g.["X_H",..] or leave empty to not use abundances.
abundance_labels: ["Ti_Fe"]

base_labels: ["teff", "logg", "feh",]

#------------------------------------------------------------------------------
# Diagnostic Plot Settings
#------------------------------------------------------------------------------
# Models are currently saved  with N_px in the filename.
# TODO: come up with a more informative/robust labelling scheme.
npx: 5024

# The grating at which the WiFeS B3000 spectra transition to R7000 spectra to
# use when plotting separate 'b' and 'r' plots.
wl_grating_changeover: 5400

# If this is true, we only plot the first order/linear theta coefficients. If
# False, we include extra panels for the cross and quadtratic terms
only_plot_first_order_coeff: False

# Line lists to overplot against theta coefficients. Due to the density of
# atomic features in the blue, we have a more strict threshold for labelling.
line_list_file: "data/t3500_g+5.0_z+0.00_a+0.00_v1.00_latoms.eqw"
ew_min_ma_b: 400
ew_min_ma_r: 150

# Which species to overplot on our theta plot. It gets very busy the more
# species we plot, so it's currently limited to the most prominent species.
species_to_plot: ["Ca 1", "Ti 1", "Fe 1"]

# Adopted label uncertainties and systematics based on cross validation
# performance on the benchmark set. Quoted systematics are fit - lit, meaning
# that a positive systematic means the Cannon has *overestimated* the value 
# (and thus the systematic should be substracted). The temperature systematic 
# and uncertainty will be adopted from just the interferometric set, whereas
# logg and [Fe/H] will be taken from the complete sample. 
# TODO: adopted_label_uncertainties = sm.adopted_label_uncertainties
adopted_label_systematics: [-4, 0.0, 0.00, 0.05,]
adopted_label_uncertainties: [58, 0.04, 0.10, 0.06]

# Here we've picked a set of spectral types ranging over our BP-RP range.
representative_stars_source_ids: 
- "5853498713190525696"      # M5.5, GJ 551
- "2640434056928150400"      # M5, GJ 1286
- "2595284016771502080"      # M5, LHS 3799
- "2868199402451064064"      # M4.7, GJ 1288
- "6322070093095493504"      # M2, GJ 581
- "2603090003484152064"      # M3, GJ 876
- "4472832130942575872"      # M4, Gl 699
- "2910909931633597312"      # M3, LP 837-53
- "3184351876391975808"      # M2, Gl 173
- "2739689239311660672"      # M0, Gl 908
- "145421309108301184"       # K8, Gl 169
- "4282578724832056576"      # M0.7, Gl 740
- "3266980243936341248"      # M1V, NLTT 10349, most metal poor

# Threshold at which to flag logg values in our results tabls
aberrant_logg_threshold: 0.075

#------------------------------------------------------------------------------
# Model Comparison Settings
#------------------------------------------------------------------------------
# For comparison purposes, we select one trained 3 label model, and one
# trained 4 label model to compare performance via compare_stannon_models.py
sm_1_path: "spectra/stannon_model_basic_3label_5024px_teff_logg_feh.pkl"
sm_2_path: "spectra/stannon_model_basic_4label_5024px_teff_logg_feh_Ti_Fe.pkl"

sm_1_label: "3 Label"
sm_2_label: "4 Label"

# Fractional thresholds to use when comparing Cannon vs MARCS spectra
delta_thresholds: [0.1, 0.05, 0.02]